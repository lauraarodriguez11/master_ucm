{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un ejemplo de proceso de puntos (point pattern)\n",
    "\n",
    "En **geoestadística** y en **econometría espacial**, se considera el espacio como un dominio en el que a cada punto del espacio le corresponde un atributo, siendo las localizaciones de los objetos espaciales fijas y donde se espera que \"todas\" las localizaciones tengan un valor.   \n",
    "  -  La Geoestadística trata fundamentalmente de hacer predicciones sobre el valor de un atributo en un punto no observado utilizando la información de otros puntos donde si se ha muestreado información sobre dicho atributo. Por ejemplo datos de precipitaciones o de presión atmosférica, o de contaminación. Se hace un muestreo de los datos de en algunos puntos del espacio (donde existen estaciones meteorilógicas o de medición), y se busca **interpolar** esos datos a puntos intermedios donde no se dispone de datos.\n",
    "  -  En Econometría normalmente se trabajan con polígonos, es decir, los datos espaciales son en realidad, polígonos que representan diferentes estructural geopolíticas, como municipios, países, distritos, provincias, códigos postales, y se tiene información sobre todos ellos. EL objetivo en econometría espacial no es tanto la interpolación para predecir valores como la **modelización de la posible dependencia o aoutocorrelación espacial**: relación o dependencia del valor de un atributo en una regíon respecto al valor que toma ese mismo atributo en sus vecinos más cercanos.\n",
    "  \n",
    "   También es posible trabajar con modelos de regresión espacial con datos de puntos, por ejemplo precios de la vivienda, pero nuevamente el objetivo es modelizar la depedencia espacial, más que la interpolaión (aunque claramente aquí es donde geoestadísitca y regresión espacial acaban convergiendo, ya que en ambos tipos de modelización, por vías diferentes se intenta alcanzar el mismo objetivo, predecir el valor de un avivienda).\n",
    "\n",
    "  - En ocasiones nos enfrentamos a datos espaciales donde los propios puntos en el espacio para los que se tiene información són el objeto de estudio. En estos casos los puntos observados se consideran sucesos que podrían tener lugar en varios lugares pero que sólo ocurren en unos pocos, una colección de tales sucesos se denomina **proceso de puntos o point pattern.** Un excelente resumen de este tipo de datos puede encontrarse en el libro de [Sergio Rey et al. Geographic Data Science with Python](https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html)\n",
    "\n",
    "\n",
    "Un ejemplo de estos procesos de puntos donde a localización de los puntos es uno de los aspectos clave de interés para el análisis es por ejemplo la ubicación de las recogidas de clientes de UBER . En esta práctica voy a utilizar los datos de recogidas de [UBER en la ciudad de Nueva York que están disponibles en Kagle](https://www.kaggle.com/datasets/tekbahadurkshetri/uber-clustering). Para vuestra comodidad os he dejado los datos en el archivo \"uber_clean.csv\".\n",
    "\n",
    "Con este tipo de procesos de puntos puede ser interesante analizar la propia **frecuencia de localizaciones donde aparecen los datos**, para detectar, por ejemplo, puntos calientes (¿por qué los eventos ocurren en algunal localizaciones y no en otras?), y para comprobar si hay algún **patrón espacial en la localización de los eventos**, o si por el contrario puede suponerse una **distribución espacial de los puntos completamente aleatoria**.\n",
    "\n",
    "Los patrones de puntos pueden estar **marcados**, si se proporcionan más atributos con la ubicación, o **sin marcar**, si sólo se proporcionan las coordenadas del lugar donde se produjo el suceso.\n",
    "\n",
    "Nosotros utilizaremos los datos también para realizar un análisis de **cluster o agrupación de datos espaciales** con el objetivo de poder dividir la ciudad de Nueva York en diferentes subzonas de negocio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import dbscan\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import contextily\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from pointpats import centrography\n",
    "\n",
    "#conda install -c conda-forge folium\n",
    "import folium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo los datos desde el archivo 'uber_clean.csv', de la carpeta datos, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datos/uber_clean.csv')  \n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que en este conjunto de datos sólo se dispone de información del punto de recogida de los clientes, por lo que trataremos los datos como un proceso de puntos no marcados (la información de la base la utilizaremos posteriormente para comprobar la bondad de nuestro proceso de agrupación de datos).    \n",
    "     \n",
    "\n",
    "Para que dure algo menos de tiempo la práctica voy a hacer una subselección de sólo 50.000 registros. Vosotros podéis intentar realizar el mismo análisis con el conjunto completo de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(n=50000, frac=None, replace=False, weights=None, random_state=1234)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de patrones puntuales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x='Lon', y='Lat',data=df, alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con seaborn\n",
    "sns.jointplot(x=\"Lon\", y=\"Lat\", data=df, s=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dar cierto contexto geográfico podemos por ejemplo utilizar la **librería contextily**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_axes = sns.jointplot(\n",
    "    x=\"Lon\", y=\"Lat\", data=df, s=0.5\n",
    ")\n",
    "contextily.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podríamos utilizar mapas dinámicos con leaflet utilizando la librería folium. Que permite realizar de forma sencilla una exploración del mapa de Open StreetMap    \n",
    "    \n",
    "Primero seleccione el mapa sobre el que voy a representar los puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = folium.Map(location=[40.7128, -74.0060], zoom_start=10,tiles = \"openstreetmap\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora represento los puntos en el mapa anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "# como son muchos puntos, solo voy a mostrar los 2000 primeros\n",
    "for i in range(0,2000):\n",
    "    folium.CircleMarker(\n",
    "        location=[df.iloc[i]['Lat'], df.iloc[i]['Lon']],\n",
    "        radius=5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapa de densidad de frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen zonas donde la densidad de puntos es tan alta que sólo se ve algo realizando un zoom. Para estos casos resulta muy util hacer un histograma de de frecuencias utilizando una función de densidad para la longitud y la latitud (mapas de calor), conocidos como histogramas 2-dimensionales o espaciales.\n",
    "\n",
    "La construcción de estos histogramas espaciales se realiza estableciendo una cuadrícula regular (cuadrada o hexagonal) sobre el mapa y posteriormente contando cuántos puntos hay en cada celda de la cuadrícula. Después se presentan los recuentos como lo haríamos con cualquier otro mapa de coroplóticos o de coropletas. \n",
    "\n",
    "Aquí utilizo la tramificación o binning hexagonal (a veces llamado hexbin) porque tiene propiedades ligeramente mejores que las cuadrículas cuadradas, como una menor distorsión de la forma y una conectividad más regular entre las celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamos el gráfico\n",
    "f, ax = plt.subplots(1, figsize=(12, 9))\n",
    "# Generamos y añadimos la retícula hexagonal con 500 hexagononos\n",
    "hb = ax.hexbin(\n",
    "    df[\"Lon\"],\n",
    "    df[\"Lat\"],\n",
    "    gridsize=500,\n",
    "    linewidths=0,\n",
    "    alpha=0.5,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels,\n",
    "    )\n",
    "# añadimos la barra de color\n",
    "plt.colorbar(hb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtro para utilizar sólo datos de la area de Manhattan\n",
    "df_filtered = df[(df['Lon'] >= -74.05) & (df['Lon'] <= -73.85) & (df['Lat'] >= 40.66) & (df['Lat'] <= 40.82)]\n",
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(12, 9))\n",
    "# Generamos y añadimos la retícula hexagonal con 50 hexagononos\n",
    "hb = ax.hexbin(\n",
    "    df_filtered[\"Lon\"],\n",
    "    df_filtered[\"Lat\"],\n",
    "    gridsize=50,\n",
    "    linewidths=0,\n",
    "    alpha=0.5,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels,\n",
    "    )\n",
    "# añadimos la barra de color\n",
    "plt.colorbar(hb)\n",
    "# eliminamos los ejes\n",
    "#ax.set_axis_off()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos hacer el histograma espacial utilizando una **función de densidad Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtro para utilizar sólo datos de la area de Manhattan\n",
    "# df_filtered = df[(df['Lon'] >= -74.05) & (df['Lon'] <= -73.85) & (df['Lat'] >= 40.66) & (df['Lat'] <= 40.82)]\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Generamos la función de densidad de kernel\n",
    "sns.kdeplot(\n",
    "    x=\"Lon\",\n",
    "    y=\"Lat\",\n",
    "    data=df_filtered,\n",
    "    n_levels=100,\n",
    "    fill=True,\n",
    "    alpha=0.5,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La centrografía es el análisis de la centralidad en un patrón de puntos. Por **centralidad** entendemos la ubicación general y la dispersión del patrón. Si el hexágono anterior puede verse como un **histograma espacial**, la centrografía es el equivalente en patrones puntuales de las **medidas de tendencia central, como la media**.    \n",
    "    \n",
    "Estas medidas son útiles porque nos permiten **resumir las distribuciones espaciales** en conjuntos de información más pequeños (por ejemplo, un solo punto). En la centrografía se utilizan muchos índices diferentes para proporcionar una indicación de **dónde** se encuentra un patrón de puntos, la **intensidad** con la que el patrón de puntos se agrupa en torno a su centro o lo **irregular de su forma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medida de tendencia central: \"centro de masa\" \n",
    "\n",
    "\n",
    "Para patrones puntuales no marcados esta tendencia central o centro de masa sería la media de las coordenadas; para patrones de puntos markados sería el punto central de los datos con mayor valor de su atributo marcado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medida de tendencia central: \"centro de masa\" (para patrones puntuales no marcados sería la media de las coordenadas, para patrones de puntos markados sería el punto central de los datos con mayor valor de su atributo marcado)\n",
    " \n",
    "from pointpats import centrography\n",
    "\n",
    "mean_center = centrography.mean_center(df[[\"Lon\", \"Lat\"]])\n",
    "mediana_center = centrography.euclidean_median(df[[\"Lon\", \"Lat\"]])\n",
    "\n",
    "print(mean_center, mediana_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a representar la tendencia central\n",
    "joint_axes = sns.jointplot(\n",
    "    x=\"Lon\", y=\"Lat\", data=df, s=0.75, height=9\n",
    ")\n",
    "# Añado punto medio\n",
    "joint_axes.ax_joint.scatter(\n",
    "    *mean_center, color=\"red\", marker=\"x\", s=50, label=\"Centro Medio\"\n",
    ")\n",
    "joint_axes.ax_marg_x.axvline(mean_center[0], color=\"red\")\n",
    "joint_axes.ax_marg_y.axhline(mean_center[1], color=\"red\")\n",
    "# Añado punto mediano\n",
    "joint_axes.ax_joint.scatter(\n",
    "    *mediana_center,\n",
    "    color=\"limegreen\",\n",
    "    marker=\"o\",\n",
    "    s=50,\n",
    "    label=\"Centro Mediano\"\n",
    ")\n",
    "joint_axes.ax_marg_x.axvline(mediana_center[0], color=\"limegreen\")\n",
    "joint_axes.ax_marg_y.axhline(mediana_center[1], color=\"limegreen\")\n",
    "# Legend\n",
    "joint_axes.ax_joint.legend()\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    ")\n",
    "# Clean axes\n",
    "joint_axes.ax_joint.set_axis_off()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo hago sólo para el centro de Manhattan\n",
    "mean_center = centrography.mean_center(df_filtered[[\"Lon\", \"Lat\"]])\n",
    "mediana_center = centrography.euclidean_median(df_filtered[[\"Lon\", \"Lat\"]])\n",
    "\n",
    "print(mean_center, mediana_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a representar la tendencia central\n",
    "joint_axes = sns.jointplot(\n",
    "    x=\"Lon\", y=\"Lat\", data=df_filtered, s=0.75, height=9\n",
    ")\n",
    "# Añado punto medio\n",
    "joint_axes.ax_joint.scatter(\n",
    "    *mean_center, color=\"red\", marker=\"x\", s=50, label=\"Centro Medio\"\n",
    ")\n",
    "joint_axes.ax_marg_x.axvline(mean_center[0], color=\"red\")\n",
    "joint_axes.ax_marg_y.axhline(mean_center[1], color=\"red\")\n",
    "# Añado punto mediano\n",
    "joint_axes.ax_joint.scatter(\n",
    "    *mediana_center,\n",
    "    color=\"limegreen\",\n",
    "    marker=\"o\",\n",
    "    s=50,\n",
    "    label=\"Centro Mediano\"\n",
    ")\n",
    "joint_axes.ax_marg_x.axvline(mediana_center[0], color=\"limegreen\")\n",
    "joint_axes.ax_marg_y.axhline(mediana_center[1], color=\"limegreen\")\n",
    "# Legend\n",
    "joint_axes.ax_joint.legend()\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    ")\n",
    "# Clean axes\n",
    "joint_axes.ax_joint.set_axis_off()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo hago ahora con el grtáfico de densidad\n",
    "\n",
    "    \n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Generamos la función de densidad de kernel\n",
    "sns.kdeplot(\n",
    "    x=\"Lon\",\n",
    "    y=\"Lat\",\n",
    "    data=df_filtered,\n",
    "    n_levels=100,\n",
    "    fill=True,\n",
    "    alpha=0.5,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "# Añado punto medio\n",
    "ax.scatter(\n",
    "    *mean_center, color=\"red\", marker=\"x\", s=50, label=\"Centro Medio\"\n",
    ")\n",
    "# Añado punto mediano\n",
    "    \n",
    "ax.scatter(\n",
    "    *mediana_center,\n",
    "    color=\"limegreen\",\n",
    "    marker=\"o\",\n",
    "    s=50,\n",
    "    label=\"Centro Mediano\"\n",
    ")\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "# añadimos la leyenda\n",
    "ax.legend()\n",
    "# eliminamos los ejes\n",
    "ax.set_axis_off()\n",
    "# Display\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medida de dispersión: \"distancia típica\" (standard distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta medida proporciona la distancia media al centro de la nube de puntos (como la medida por el centro de masa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrography.std_distance(df[[\"Lon\", \"Lat\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que, por término medio, los uber se toman a unos 0.0726 grados equivalente a unos (0.0726*40000/360)~=8 kilómetros del centro medio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar esta \"distancia típica\" se utiliza la **elipse típica o la elipse de desviación típica** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major, minor, rotation = centrography.ellipse(df_filtered[[\"Lon\", \"Lat\"]])\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# prerparo el gráfico\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# dibujo pos pontos t los centros de masa\n",
    "ax.scatter(df_filtered[\"Lon\"], df_filtered[\"Lat\"], s=0.75)\n",
    "ax.scatter(*mean_center, color=\"red\", marker=\"x\", label=\"Centro Medio\")\n",
    "ax.scatter(\n",
    "    *mediana_center, color=\"limegreen\", marker=\"o\", label=\"Centro Mediano\"\n",
    ")\n",
    "\n",
    "# creo la elipse\n",
    "ellipse = Ellipse(\n",
    "    xy=mean_center,  # center the ellipse on our mean center\n",
    "    width=major * 2,  # centrography.ellipse only gives half the axis\n",
    "    height=minor * 2,\n",
    "    angle=np.rad2deg(\n",
    "        rotation\n",
    "    ),  # Angles for this are in degrees, not radians\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Std. Ellipse\",\n",
    ")\n",
    "ax.add_patch(ellipse)\n",
    "\n",
    "ax.legend()\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delimitación de áreas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer un ejercicio de delimitación de áreas. Fíjate que dentro del área de Manhattan hay actuando UBERs que pertenecen a CincoBases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = df_filtered['Base'].value_counts()\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a representarlas con diferentes colores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerparo el gráfico\n",
    "plt.figure(figsize=(10, 6))\n",
    "# dibujo pos pontos t los centros de masa\n",
    "ax=sns.scatterplot(x='Lon', y='Lat', hue='Base', data=df_filtered, palette='tab10', s=10)\n",
    "ax.legend()\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "\n",
    "plt.title('recogidas según Base de UBER')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a intentar delimitar el área de actuación de los uber de la base B02682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = df_filtered[df_filtered['Base'] == \"B02682\"]\n",
    "coordinates = user[[\"Lon\", \"Lat\"]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convex hull**     \n",
    "En primer lugar, calcularemos el casco convexo (**covex hull**), que es la forma convexa más cerrada que encierra todos los viajes de UBER de la Base B028682. Por convexa, entendemos que la forma nunca se «dobla» sobre sí misma; no tiene divisiones, valles, crenulaciones ni agujeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convex_hull_vertices = centrography.hull(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# prerparo el gráfico\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "\n",
    "# Representar los datos de coordinates seleccionados\n",
    "ax.scatter(coordinates[:, 0], coordinates[:, 1], s=1, c='red', label='Datos B02682')\n",
    "\n",
    "# Representar el polígono del convex_hull_vertices\n",
    "hull_polygon = plt.Polygon(convex_hull_vertices, edgecolor='green', fill=None, label='Convex Hull')\n",
    "\n",
    "# Añadir el polígono al gráfico\n",
    "ax.add_patch(hull_polygon)\n",
    "\n",
    "# Añadir leyenda\n",
    "plt.legend()\n",
    "\n",
    "# añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Representación de datos y Convex Hull')\n",
    "plt.xlabel('Lon')\n",
    "plt.ylabel('Lat')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alpha shape**    \n",
    "En segundo lugar, puede calcularse la denominada **forma alfa** ((alpha shape)), que puede entenderse como una versión *más ajustada* del casco convexo. Una forma de pensar en una forma alfa es como el espacio creado al rodar una pelota pequeña alrededor de la forma. Como la pelota es más pequeña, rueda en los desniveles y valles creados entre los puntos. A medida que la pelota crece, la forma alfa se convierte en el casco convexo. Pero, para las bolas pequeñas, la forma puede ser muy estrecha. De hecho, si alfa se hace demasiado pequeña, se «desliza» por los puntos, ¡dando lugar a más de un casco!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import libpysal\n",
    "\n",
    "alpha_shape, alpha, circs = libpysal.cg.alpha_shape_auto(\n",
    "    coordinates, return_circles=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "# Plot a green alpha shape\n",
    "gpd.GeoSeries(\n",
    "    [alpha_shape]\n",
    ").plot(\n",
    "    ax=ax,\n",
    "    edgecolor=\"green\",\n",
    "    facecolor=\"green\",\n",
    "    alpha=0.2,\n",
    "    label=\"Forma alfa\",\n",
    ")\n",
    "\n",
    "# Dibujamos los puntos \n",
    "ax.scatter(\n",
    "    *coordinates.T, color=\"k\", marker=\".\", label=\"Datos B02682\"\n",
    ")\n",
    "\n",
    "# dibujamos los círculos que forman la frontera de la forma alpha \n",
    "for i, circle in enumerate(circs):\n",
    "    # sólo etiquetamos el primero de los círculos\n",
    "    if i == 0:\n",
    "        label = \"Bounding Circles\"\n",
    "    else:\n",
    "        label = None\n",
    "        ax.add_patch(\n",
    "            plt.Circle(\n",
    "                circle,\n",
    "                radius=alpha,\n",
    "                facecolor=\"none\",\n",
    "                edgecolor=\"r\",\n",
    "                label=label,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "# añadimos convex hull\n",
    "ax.add_patch(\n",
    "    plt.Polygon(\n",
    "        convex_hull_vertices,\n",
    "        closed=True,\n",
    "        edgecolor=\"blue\",\n",
    "        facecolor=\"none\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "        label=\"Convex Hull\",\n",
    "    )\n",
    ")\n",
    "\n",
    "## añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Dejo como ejercicio que representéis conjuntamente el convex Hull ( y el alpha shape) de las cinco bases para detenctar diferencias en las áreas de actuación de cada una de ellas.     \n",
    "\n",
    "\n",
    "- Otro ejercicio podría ser intentar detectar diferencias tanto en la distribución de puntos (gráfico de densidad) y en área de actuación de los días entre diario y de los fines de semana (¿se aprecia alguna diferencia?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aleatoriedad y concentración espacial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los objetivos del análisis de procesos de puntos es averiguar si los eventos aparecen de forma totalmente aleatoria sobre el espacio o si por el contrario existe cierto patrón espacial que haga que los eventos sucedan unos cerca de otros, formando concnetraciones de eventos, o agrupamiento de eventos. Cuando vamos los modelos de regresión espacial volveremos sobre esta idea de distribución espacial, y de la existencia de patrones de comportamiento espacial. Ahora hablamos de patrones espaciales en la localización de eventos, y cuando pasemos a los modelos de autocorrelación espacial hablaremos de dependencia espacial de los valores de un atributo respecto a los valores de ese atributo en las regiones vecinas.    \n",
    "\n",
    " \n",
    "Para detectar estos patrones espaciales existen diferentes téncinas. El primer conjunto de técnicas se denominan **estadísticas de cuadrantes** y reciben su nombre por su planteamiento de dividir los datos en pequeñas áreas (cuadrantes). Una vez creados, estos «cubos» se utilizan para examinar la uniformidad de los recuentos en ellos. El segundo conjunto de técnicas procede de Ripley (1988) y consiste en medir la distancia entre puntos en un patrón de puntos.\n",
    "\n",
    "\n",
    "A efectos comparativos, resulta útil proporcionar el patrón derivado de un proceso completamente aleatorio desde el punto de vista espacial. Es decir, la ubicación y el número de puntos son totalmente aleatorios; no hay ni agrupación ni dispersión. En el análisis de patrones de puntos, esto se conoce como proceso de puntos de Poisson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from pointpats import (\n",
    "    distance_statistics,\n",
    "    QStatistic,\n",
    "    random,\n",
    "    PointPattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulación de datos aleatorios en relación a los datos de la base B02682 (que hemos almacenado en \"coordinates\")\n",
    "random_pattern = random.poisson(coordinates, size=len(coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "plt.scatter(\n",
    "    *coordinates.T,\n",
    "    color=\"k\",\n",
    "    marker=\".\",\n",
    "    label=\"datos de la base B02682 \"\n",
    ")\n",
    "plt.scatter(*random_pattern.T, color=\"r\", marker=\"x\", alpha=0.5, label=\"Proceso Aleatorio Espacial\")\n",
    "## añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "ax.legend(ncol=1, loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución aleatoria se ha hecho por defecto en una frontera cuadrada, si quisiesemos utilizar otro tipo de forma (convex hull o alpha shape) también podríamos hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pattern_ashape = random.poisson(\n",
    "    alpha_shape, size=len(coordinates)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "plt.scatter(\n",
    "    *coordinates.T,\n",
    "    color=\"k\",\n",
    "    marker=\".\",\n",
    "    label=\"datos de la base B02682 \"\n",
    ")\n",
    "plt.scatter(*random_pattern_ashape.T, color=\"r\", marker=\"x\", alpha=0.5, label=\"Proceso Aleatorio Espacial\")\n",
    "## añadimos el mapa base\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "ax.legend(ncol=1, loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrat statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estadísticas de cuadrantes (quadrat statistics) analizan la distribución espacial de los puntos en función del recuento de observaciones que caen dentro de una celda determinada. Al examinar si las observaciones se distribuyen uniformemente por el conjunto de celdas, el enfoque de cuadrantes pretende estimar si los puntos están dispersos o si están agrupados en unas pocas celdas. En sentido estricto, la estadística de cuadrantes examina la uniformidad de la distribución en las celdas mediante una prueba estadística X2 habitual en el análisis de contingencias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat = QStatistic(coordinates)\n",
    "qstat.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-valor del test de Chi2 para la Ho de distribución uniforme entre todos los cuadrantes\n",
    "qstat.chi2_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar vemos los resultados con el proceso totalmente aleatorio que hemos estimado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat_null = QStatistic(random_pattern)\n",
    "qstat_null.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat_null.chi2_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con este enfoque es que supone que todos los puntos en el cuadrado son susceptibles de ubicar un evento del proceso. Y como es claro en nuestro caso, los Uber no pueden recoger a clientes en el mar. Por eso hay que seguir analizando otro tipo de técnicas.    \n",
    "\n",
    "Igualmente, el hecho de que los recuentos de cuadrantes se midan en un mosaico regular que se superpone a la extensión potencialmente irregular de nuestro patrón puede inducirnos a error. En particular, los patrones irregulares pero aleatorios pueden ser erróneamente considerados «significativos» por este enfoque. Consideremos nuestro conjunto aleatorio generado dentro del polígono de forma alfa con la cuadrícula de cuadrantes superpuesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat_null_ashape = QStatistic(random_pattern_ashape)\n",
    "qstat_null_ashape.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat_null_ashape.chi2_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que estamos rechazando la hipótesis nual de aleatoriedad, auqnue sabemos que por contrcción la distribución es totalmente aleatoria en el alpha shape. En conclusión este tipo de estadísticas de cuadrantes sólo puede considerarse como válidads en supoerficies regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ripley’s alphabet of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo grupo de estadísticas espaciales que consideramos se centra en las distribuciones de dos cantidades en un patrón de puntos: las distancias a los vecinos más próximos y lo que denominaremos «huecos» en el patrón. Se derivan del trabajo seminal de Ripley(1991) sobre cómo caracterizar la agrupación o co-localización en patrones de puntos. Cada una de ellas caracteriza un aspecto del patrón de puntos a medida que aumentamos el rango de distancia desde cada punto para calcularlas.    \n",
    "\n",
    "(Brian D Ripley. Statistical inference for spatial processes. Cambridge University Press, 1991.)\n",
    "\n",
    "La primera función de Ripley se centra en la distribución de las **distancias a los vecinos más próximos**. Esta función resume las distancias entre cada punto y su vecino más cercano. **La G de Ripley** calcula la proporción de puntos en los que el vecino más próximo se encuentra dentro de un umbral de distancia determinado y traza el porcentaje acumulado en función de los radios de distancia crecientes. La distribución de estos porcentajes acumulativos tiene una forma característica en procesos completamente aleatorios desde el punto de vista espacial.     \n",
    "      \n",
    "La intuición que subyace a la G de Ripley es la siguiente: podemos saber hasta qué punto nuestro patrón se parece a uno espacialmente aleatorio calculando la distribución acumulativa de las distancias a los vecinos más próximos sobre umbrales de distancia crecientes y comparándola con la de un conjunto de patrones simulados que siguen un proceso espacialmente aleatorio conocido. Normalmente, se utiliza un proceso espacial de Poisson como distribución de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "coordinates_sample = coordinates[np.random.choice(coordinates.shape[0], 500, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de la librería pointpats \n",
    "# como tarda mucho (son más de 11 mil localizaciones las que tiene que analizar) voy a hacer una selección aleatoria sólo de 500 puntos \n",
    "np.random.seed(1234)\n",
    "coordinates=coordinates[np.random.choice(coordinates.shape[0], 500, replace=False)]\n",
    "\n",
    "g_test = distance_statistics.g_test(\n",
    "    coordinates, support=40, keep_simulations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensando en estas distribuciones de distancias, un patrón «agrupado» debe tener más puntos cercanos entre sí que un patrón «disperso»; y un patrón completamente aleatorio debería tener algo intermedio. Por lo tanto, si la función aumenta rápidamente con la distancia, probablemente estemos ante un patrón agrupado. Si aumenta lentamente con la distancia, tenemos un patrón disperso. Algo intermedio será difícil de distinguir del puro azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(\n",
    "    1, 2, figsize=(9, 3), gridspec_kw=dict(width_ratios=(6, 3))\n",
    ")\n",
    "# plot all the simulations with very fine lines\n",
    "ax[0].plot(\n",
    "    g_test.support, g_test.simulations.T, color=\"k\", alpha=0.01\n",
    ")\n",
    "# and show the average of simulations\n",
    "ax[0].plot(\n",
    "    g_test.support,\n",
    "    np.median(g_test.simulations, axis=0),\n",
    "    color=\"cyan\",\n",
    "    label=\"median simulation\",\n",
    ")\n",
    "\n",
    "\n",
    "# and the observed pattern's G function\n",
    "ax[0].plot(\n",
    "    g_test.support, g_test.statistic, label=\"observed\", color=\"red\"\n",
    ")\n",
    "\n",
    "# clean up labels and axes\n",
    "ax[0].set_xlabel(\"distance\")\n",
    "ax[0].set_ylabel(\"% of nearest neighbor\\ndistances shorter\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(r\"Ripley's $G(d)$ function\")\n",
    "\n",
    "# plot the pattern itself on the next frame\n",
    "ax[1].scatter(*coordinates.T,marker=\".\", s=0.5)\n",
    "contextily.add_basemap(\n",
    "    ax[1],\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "\n",
    "# and clean up labels and axes there, too\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_title(\"Pattern\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el gráfico muestra un crecimiento del porcentaje acumulado de vecinos más rápido que el correspondiente a una distirbución aleatoria, por lo que podemos concluir que existen patrones espaciales de concentración de eventos.Existen otras técinas y funciones (como la función F de Ripley), pero que quedal fuera del alcance de esta clase. véase [Rey et al , 2023](https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html#centrography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificación de las agrupación o Clustering (DBSCAN)      \n",
    "Las dos secciones anteriores sobre el análisis espacial exploratorio de patrones de puntos proporcionan métodos para caracterizar si los patrones de puntos están dispersos o agrupados en el espacio. Otra forma de ver el contenido de esas secciones es que nos ayudan a explorar el grado de agrupación general. Sin embargo, saber que un patrón de puntos está agrupado no nos da necesariamente información sobre dónde reside ese (conjunto de) conglomerado(s). Para ello, necesitamos pasar a un método capaz de identificar zonas de alta densidad de puntos dentro de nuestro patrón. En otras palabras, en esta sección nos centramos en la existencia y localización de los clusters.\n",
    "\n",
    "De los muchos algoritmos de agrupamiento espacial de puntos, vamos a utilizar el llamado DBSCAN (Density-Based Spatial Clustering of Applications) [EKS+96]. DBSCAN es un algoritmo ampliamente utilizado que se originó en el área de descubrimiento de conocimiento y aprendizaje automático y que desde entonces se ha extendido a muchas áreas, incluyendo el análisis de puntos espaciales.\n",
    "\n",
    "Referencia: Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, and others. A density-based algorithm for discovering clusters in large spatial databases with noise. In Kdd, volume 96, 226–231. 1996."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el punto de vista de DBSCAN, un cluster es una concentración de al menos **m** puntos, cada uno de ellos a una distancia de **r** de al menos otro punto del cluster. Siguiendo esta definición, el algoritmo clasifica cada punto de nuestro patrón en tres categorías:\n",
    "\n",
    " - Ruido, para aquellos puntos fuera de un cluster.\n",
    "\n",
    " - Núcleos, para aquellos puntos dentro de un cluster con al menos **m** puntos en el cluster dentro de la distancia **r**.\n",
    "\n",
    "\n",
    " - Fronteras, para los puntos dentro de un clúster con menos de m puntos en el clúster dentro de la distancia r.     \n",
    "\n",
    "La flexibilidad (pero también algunas de las limitaciones) del algoritmo residen en que tanto **m** como **r** deben ser especificados por el usuario antes de ejecutar DBSCAN. Se trata de un punto crítico, ya que su valor puede influir significativamente en el resultado final. Antes de profundizar en este tema, vamos a hacer un primer intento de calcular DBSCAN en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clusterer = DBSCAN()\n",
    "# Fit to our data\n",
    "clusterer.fit(df_filtered[[\"Lon\", \"Lat\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo labels_ tiene siempre la misma longitud que el número de puntos utilizados para ejecutar DBSCAN. Cada valor representa el índice del cluster al que pertenece un punto. Si el punto se clasifica como ruido, recibe un -1. Para facilitar las cosas más adelante, convirtamos las etiquetas en un objeto Serie que podamos indexar del mismo modo que nuestra colección de puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = pd.Series(clusterer.labels_, index=df_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora podemos visualizar los clusters\n",
    "# Setup figure and axis\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Subset points that are not part of any cluster (noise)\n",
    "noise = df_filtered.loc[lbls == -1, [\"Lon\", \"Lat\"]]\n",
    "\n",
    "# Plot noise in grey\n",
    "ax.scatter(noise[\"Lon\"], noise[\"Lat\"], c=\"black\", s=10, linewidth=0)\n",
    "\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points (tw) and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    df_filtered.loc[df_filtered.index.difference(noise.index), \"Lon\"],\n",
    "    df_filtered.loc[df_filtered.index.difference(noise.index), \"Lat\"],\n",
    "    c=\"red\",\n",
    "    marker=\".\", \n",
    "    s=0.5,\n",
    "    linewidth=0,\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No parece muy satisfactorio, porque los valores por defecto de DBSCAN es un radio de 0.5 y m=5 puntos     \n",
    "\n",
    "Vamos a cambiarlo para que el número de puntos sea un 1% del total de puntos. y que la distancia sea 0.5 veces la distancia típica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the number of points 5% of the total represents\n",
    "minp = np.round(df_filtered.shape[0] * 0.05)\n",
    "minp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.25, 0.5, o 0.75  veces la distancia típica\n",
    "radio= centrography.std_distance(df_filtered[[\"Lon\", \"Lat\"]])*0.5\n",
    "radio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun DBSCAN\n",
    "clusterer = DBSCAN(eps=radio, min_samples=int(minp))\n",
    "clusterer.fit(df_filtered[[\"Lon\", \"Lat\"]])\n",
    "\n",
    "# Turn labels into a Series\n",
    "lbls = pd.Series(clusterer.labels_, index=df_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "\n",
    "# Subset points that are not part of any cluster (noise)\n",
    "noise = df_filtered.loc[lbls == -1, [\"Lon\", \"Lat\"]]\n",
    "\n",
    "# Plot noise in grey\n",
    "ax.scatter(noise[\"Lon\"], noise[\"Lat\"], c=\"grey\", s=5, linewidth=0)\n",
    "\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    df_filtered.loc[df_filtered.index.difference(noise.index), \"Lon\"],\n",
    "    df_filtered.loc[df_filtered.index.difference(noise.index), \"Lat\"],\n",
    "    c=\"red\",\n",
    "     marker=\".\", \n",
    "    s=0.5,\n",
    "    linewidth=0,\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=\"EPSG:4326\",  # hemos especificado el sistema de referencia internacional WGS84\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    "    )\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En gris aparecerían las ubicaciones que se consideran no agrupadas, y en rojo las que sí están agrupadas... podríamos seguir trabajando sólo con estas, pero de momento lo dejamos aquí, esto es, con la identificación de los puntos que pueden considerarse agrupados o concentrados según el patrón \"desconocido\" de distribución espacial de los puntos de recogida de UBER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
