# -*- coding: utf-8 -*-
"""ClusteringCode__24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D6ovMd9WjZMEH3Nj1W_EJnyrAXeLMAPM

## EJEMPLO "PAÍSES Y ESPERANZA DE VIDA"
"""

#!pip install --upgrade scipy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Definimos nuestro entorno de trabajo.
import os
os.chdir(r'C:\Users\lrodr\OneDrive\Documentos\master_ucm\trabajos\8')

# Read the Excel file into a DataFrame
df = pd.read_excel('Esperanzadevida.xlsx')

# View the first few rows of the DataFrame
df.head()

print(df)

"""Vamos a crear un mapa de calor (con un clustering jerárquico básico incluido) para hacer un análisis exploratio de las variables por países"""

import seaborn as sns

# Set 'PAIS' as the index
df = df.set_index('PAIS')
# Extract the index labels
PAIS_labels = df.index

# Create the heatmap
#sns.heatmap(df, cmpa='coolwarm', annot=True)
#clustered heatmap
sns.clustermap(df, cmap='coolwarm', annot=True)

# Customize the plot if needed
plt.title('Heatmap with País as Categorical Variable')
plt.xlabel('Esperanza de vida a esa edad para hombres (m) y mujeres (w)')
plt.ylabel('País')

# Display the plot
plt.show()

from scipy.spatial import distance

# Calculate the pairwise Euclidean distances
distance_matrix = distance.cdist(df, df, 'euclidean')

# The distance_matrix is a 2D array containing the Euclidean distances
# between all pairs of observations.
print("Distance Matrix:")
distance_small = distance_matrix[:5, :5]
#Index are added to the distance matrix
distance_small = pd.DataFrame(distance_small, index=df.index[:5], columns=df.index[:5])

distance_small_rounded = distance_small.round(2)
print(distance_small_rounded)

df[:2]

"""#Representamos la matriz de distancias visualmente"""

plt.figure(figsize=(8, 6))
df_distance = pd.DataFrame(distance_matrix, index = df.index, columns = df.index)
sns.heatmap(df_distance, annot=False, cmap="YlGnBu", fmt=".1f")
plt.show()

"""Now is reordered"""

# Perform hierarchical clustering to get the linkage matrix
linkage = sns.clustermap(df_distance, cmap="YlGnBu", fmt=".1f", annot=False, method='average').dendrogram_row.linkage

# Reorder the data based on the hierarchical clustering
order = pd.DataFrame(linkage, columns=['cluster_1', 'cluster_2', 'distance', 'new_count']).index
reordered_data = df.reindex(index=order, columns=order)

# Optionally, you can add color bar
sns.heatmap(reordered_data, cmap="YlGnBu", fmt=".1f", cbar=False)
plt.show()

"""Standarizing the variables"""

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the DataFrame to standardize the columns
df_std = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

print(df_std)

# Calculate the pairwise Euclidean distances
distance_std = distance.cdist(df_std, df_std,"euclidean")

print(distance_std[:5,:5].round(2))

"""Recalculamos la matriz de distancias y la representamos con los datos estandarizados."""

plt.figure(figsize=(8, 6))
df_std_distance = pd.DataFrame(distance_std, index = df_std.index, columns = df.index)
sns.heatmap(df_std_distance, annot=False, cmap="YlGnBu", fmt=".1f")
plt.show()

# Perform hierarchical clustering to get the linkage matrix
linkage = sns.clustermap(df_std_distance, cmap="YlGnBu", fmt=".1f", annot=False, method='average').dendrogram_row.linkage

# Reorder the data based on the hierarchical clustering
order = pd.DataFrame(linkage, columns=['cluster_1', 'cluster_2', 'distance', 'new_count']).index
reordered_data = df.reindex(index=order, columns=order)

# Optionally, you can add color bar
sns.heatmap(reordered_data, cmap="YlGnBu", fmt=".1f", cbar=False)
plt.show()

import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

# Calculate the linkage matrix
linkage_matrix = sch.linkage(df_std_distance, method='ward')  # You can choose a different linkage method if needed

# Create the dendrogram
dendrogram = sch.dendrogram(linkage_matrix, labels=df.index, leaf_font_size=9, leaf_rotation=90)

# Display the dendrogram
plt.show()

"""# Asignamos cada observación a uno de los 4 clústeres (nos quedamos con ese número)"""

# Assign data points to 4 clusters
num_clusters = 4
cluster_assignments = sch.fcluster(linkage_matrix, num_clusters, criterion='maxclust')

# Display the cluster assignments
print("Cluster Assignments:", cluster_assignments)

# Display the dendrogram
plt.show()

"""# Añadimos la nueva variable a nustro data frame"""

# Create a new column 'Cluster' and assign the 'cluster_assignments' values to it
df['Cluster4'] = cluster_assignments

# Now 'df' contains a new column 'Cluster' with the cluster assignments

print(df["Cluster4"])

"""# Representación de los datos y su pertenencia a los clusters"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming 'df' is your original DataFrame with data
# 'cluster_assignments' contains cluster assignments

# Step 1: Perform PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df)

# Create a new DataFrame for the 2D principal components
df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Step 2: Create a scatter plot with colors for clusters
plt.figure(figsize=(10, 6))

# Loop through unique cluster assignments and plot data points with the same color
for cluster in np.unique(cluster_assignments):
    plt.scatter(df_pca.loc[cluster_assignments == cluster, 'PC1'],
                df_pca.loc[cluster_assignments == cluster, 'PC2'],
                label=f'Cluster {cluster}')
# Add labels to data points
for i, row in df_pca.iterrows():
    plt.text(row['PC1'], row['PC2'], str(df.index[i]), fontsize=8)

plt.title("2D PCA Plot with Cluster Assignments")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid()
plt.show()

"""## Clustering no jerárquico

#Kmeans
"""

from sklearn.cluster import KMeans

# Set the number of clusters (k=4)
k = 4

# Initialize the KMeans model
kmeans = KMeans(n_clusters=k, random_state=0)

# Fit the KMeans model to your standardized data
kmeans.fit(df_std)

# Get the cluster labels for your data
kmeans_cluster_labels = kmeans.labels_

print(kmeans_cluster_labels)

"""Repetimos el gráfico anterior con el k-means. ¿Será igual el gráfico?"""

# Step 2: Create a scatter plot with colors for clusters
plt.figure(figsize=(10, 6))

# Loop through unique cluster assignments and plot data points with the same color
for cluster in np.unique(kmeans_cluster_labels):
    plt.scatter(df_pca.loc[kmeans_cluster_labels == cluster, 'PC1'],
                df_pca.loc[kmeans_cluster_labels == cluster, 'PC2'],
                label=f'Cluster {cluster}')
# Add labels to data points
for i, row in df_pca.iterrows():
    plt.text(row['PC1'], row['PC2'], str(df.index[i]), fontsize=8)

plt.title("2D PCA Plot with K-means Assignments")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid()
plt.show()

"""# EL método DBSCAN"""

from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Crear el modelo DBSCAN
dbscan = DBSCAN(eps=0.1, min_samples=2)

# Ajustar el modelo a los datos
dbscan.fit(df_std)

# Obtener las etiquetas de cluster asignadas por DBSCAN
dbscan_labels = dbscan.labels_
print(dbscan_labels)

"""Los parámetros anteriores no funcionan (sale 1 sólo cluster de "ruido"). Por ello iteramos en eps y min_samples para lograr diferentes soluciones en busca de un k razonable."""

from sklearn.cluster import DBSCAN
import numpy as np

# Definir valores para eps y min_samples
eps_values = np.linspace(0.1, 1.0, 10)
min_samples_values = np.arange(2, 12)

# Inicializar variables para el número de clusters y los parámetros correspondientes
num_clusters_prev = None
eps_best = None
min_samples_best = None
num_iterations = 0

# Iterar sobre diferentes valores de eps y min_samples
for eps in eps_values:
    for min_samples in min_samples_values:
        # Crear el modelo DBSCAN
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)

        # Ajustar el modelo a los datos
        dbscan.fit(df_std)

        # Obtener el número de clusters
        num_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)

        # Mostrar el número de clusters y los parámetros de DBSCAN si cambia el número de clusters
        if num_clusters != num_clusters_prev:
            print(f"Iteración {num_iterations + 1}: eps={eps}, min_samples={min_samples}, Número de clusters={num_clusters}")
            num_clusters_prev = num_clusters
            eps_best = eps
            min_samples_best = min_samples

        num_iterations += 1
        # Parar si se alcanzan las 100 iteraciones
        if num_iterations >= 100:
            break
    if num_iterations >= 100:
        break

"""Pues vamos a coger k = 3 para poder comparar un poco con el K-means para un valor razonablemente bueno (se vio en el dendrograma...)."""

# Crear el modelo DBSCAN
dbscan = DBSCAN(eps=0.8, min_samples=2)

# Ajustar el modelo a los datos
dbscan.fit(df_std)

# Obtener las etiquetas de cluster asignadas por DBSCAN
dbscan_labels = dbscan.labels_
print(dbscan_labels)

# Creating a scatter plot with colors for clusters
plt.figure(figsize=(10, 6))

# Loop through unique cluster assignments and plot data points with the same color
for cluster in np.unique(dbscan_labels):
    plt.scatter(df_pca.loc[dbscan_labels == cluster, 'PC1'],
                df_pca.loc[dbscan_labels == cluster, 'PC2'],
                label=f'Cluster {cluster}')
# Add labels to data points
for i, row in df_pca.iterrows():
    plt.text(row['PC1'], row['PC2'], str(df.index[i]), fontsize=8)

plt.title("2D PCA Plot with DBSCAN Assignments")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid()
plt.show()

"""No tiene mucho sentido esta solución. La posible explicación sería el tamaño de la muesstra. Vemos una limitación importante del método y porque en este caso sería más robusto y daría una solución mucho más satisfactoria y razonable.

El método de Elbow para hallar el número correcto de clústeres a crear.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

#Create an array to store the WCSS values for different values of K:
wcss = []

for k in range(1, 11):  # You can choose a different range of K values
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(df_std)
    wcss.append(kmeans.inertia_)  # Inertia is the WCSS value

"""Plot the WCSS values against the number of clusters (K) and look for the "elbow" point:"""

plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

"""Otro método es el de las siluetas"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

#Create an array to store silhouette scores for different values of K

silhouette_scores = []

#Run K-means clustering for a range of K values and calculate the silhouette score for each K:

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(df_std)
    labels = kmeans.labels_
    silhouette_avg = silhouette_score(df_std, labels)
    silhouette_scores.append(silhouette_avg)

plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='-', color='b')
plt.title('Silhouette Method')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_samples

"""Run K-means clustering with the optimal number of clusters (determined using the Silhouette Method) and obtain cluster labels for each data point:"""

# Assuming 'df_std_distance' is your standardized data and '4' is the optimal number of clusters
kmeans = KMeans(n_clusters=4, random_state=0)
kmeans.fit(df_std)
labels = kmeans.labels_

"""Calculates silouhette scores for each clúster"""

silhouette_values = silhouette_samples(df_std, labels)
silhouette_values

plt.figure(figsize=(8, 6))

y_lower = 10
for i in range(4):
    ith_cluster_silhouette_values = silhouette_values[labels == i]
    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = plt.cm.get_cmap("Spectral")(float(i) / 4)
    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

plt.title("Silhouette Plot for Clusters")
plt.xlabel("Silhouette Coefficient Values")
plt.ylabel("Cluster Label")
plt.grid(True)
plt.show()

"""sort by labels para caracterizar los clusters"""

# Add the labels as a new column to the DataFrame
df_std['label'] = labels
# Sort the DataFrame by the "label" column
df_std_sort = df_std.sort_values(by="label")
# Set the 'A' column as the index
df_std = df_std.set_index(PAIS_labels)
df_std_sort['label']

# Group the data by the 'label' column and calculate the mean of each group
cluster_centroids = df_std_sort.groupby('label').mean()
cluster_centroids.round(2)
# 'cluster_centroids' now contains the centroids of each cluster

"""Lo mismo pero con los datos originales"""

# Add the labels as a new column to the DataFrame
df['label'] = labels
# Sort the DataFrame by the "label" column
df_sort = df.sort_values(by="label")

# Group the data by the 'label' column and calculate the mean of each group
cluster_centroids_orig = df_sort.groupby('label').mean()
cluster_centroids_orig.round(2)
# 'cluster_centroids' now contains the centroids of each cluster
